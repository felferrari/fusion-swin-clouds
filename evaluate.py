import argparse
import pathlib
import importlib
from conf import default, general, paths
import os
import time
import sys
from utils.dataloader import PredDataSet
from torch.utils.data import DataLoader
import torch
from tqdm import tqdm
import numpy as np
from osgeo import ogr, gdal, gdalconst
from utils.ops import save_geotiff, load_sb_image
from multiprocessing import Process
from skimage.morphology import area_opening


parser = argparse.ArgumentParser(
    description='Train NUMBER_MODELS models based in the same parameters'
)

parser.add_argument( # Experiment number
    '-e', '--experiment',
    type = int,
    default = 1,
    help = 'The number of the experiment'
)

parser.add_argument( # Number of models to be trained
    '-n', '--number-models',
    type = int,
    default = default.N_MODELS,
    help = 'The number models to be trained from the scratch'
)

parser.add_argument( # Experiment path
    '-x', '--experiments-path',
    type = pathlib.Path,
    default = paths.EXPERIMENTS_PATH,
    help = 'The patch to data generated by all experiments'
)

parser.add_argument( # Reference year to evaluate
    '-y', '--year',
    type = int,
    default = default.PROCESSED_YEARS[2],
    help = 'Reference year to evaluate'
)

parser.add_argument( # Base image to generate geotiff pred
    '-i', '--base-image-path',
    type = pathlib.Path,
    default = paths.OPT_PATH,
    help = 'The patch to base image to generate Geotiff prediction'
)

args = parser.parse_args()

exp_path = os.path.join(str(args.experiments_path), f'exp_{args.experiment}')
logs_path = os.path.join(exp_path, f'logs')
models_path = os.path.join(exp_path, f'models')
visual_path = os.path.join(exp_path, f'visual')
predicted_path = os.path.join(exp_path, f'predicted')
results_path = os.path.join(exp_path, f'results')

base_image = os.listdir(str(args.base_image_path))[0]
base_data = os.path.join(str(args.base_image_path), base_image)


outfile = os.path.join(logs_path, f'eval_{args.experiment}.txt')
with open(outfile, 'w') as sys.stdout:

    #evaluate average predictions
    #label = np.load(os.path.join(paths.PREPARED_PATH, f'{general.LABEL_PREFIX}_{args.year}.npy'))
    label = load_sb_image(os.path.join(paths.GENERAL_PATH, f'{general.LABEL_PREFIX}_{args.year}.tif'))
    tps, tns, fps, fns = [], [], [], []
    c_tps, c_tns, c_fps, c_fns = [], [], [], []
    nc_tps, nc_tns, nc_fps, nc_fns = [], [], [], []
    for im_0 in tqdm(range(general.N_IMAGES_YEAR), desc = 'Img 0'):
        for im_1 in tqdm(range(general.N_IMAGES_YEAR), desc = 'Img 1', leave = False):
            pred_sum = np.zeros_like(label, dtype=np.float16)
            for model_idx in range(args.number_models):
                pred_prob = np.load(os.path.join(predicted_path, f'{general.PREDICTION_PREFIX}_prob_{im_0}_{im_1}_{model_idx}.npy'))
                pred_sum += pred_prob
            average_pred_prob = pred_sum / args.number_models
            save_geotiff(base_data, os.path.join(visual_path, f'{general.PREDICTION_PREFIX}_avg_prob_{args.experiment}_{im_0}_{im_1}.tif'), average_pred_prob, dtype = 'float')
            np.save(os.path.join(predicted_path, f'{general.PREDICTION_PREFIX}_prob_{im_0}_{im_1}.npy'), average_pred_prob)

            pred_b = np.zeros_like(label, dtype=np.uint8)
            pred_b[average_pred_prob > 0.5] = 1
            pred_red = area_opening(pred_b, 625)

            pred_clean = pred_b
            pred_clean[label==2] = 2
            pred_clean[(pred_b - pred_red) == 1] = 2 
            label_clean = label.copy()
            label_clean[(pred_b - pred_red) == 1] = 2
            save_geotiff(base_data, os.path.join(visual_path, f'{general.PREDICTION_PREFIX}_avg_bin_{args.experiment}_{im_0}_{im_1}.tif'), pred_clean, dtype = 'byte')

            error_map = np.zeros_like(label, dtype=np.uint8)
            error_map[np.logical_and(pred_clean == 0, label_clean == 0)] = 0
            error_map[np.logical_and(pred_clean == 1, label_clean == 1)] = 1
            error_map[np.logical_and(pred_clean == 0, label_clean == 1)] = 2
            error_map[np.logical_and(pred_clean == 1, label_clean == 0)] = 3

            save_geotiff(base_data, os.path.join(visual_path, f'{general.PREDICTION_PREFIX}_error_map_{args.experiment}_{im_0}_{im_1}.tif'), error_map, dtype = 'byte')

            keep_samples = (label.flatten() != 2) #remove samples of class 2
            keep_samples[(pred_b - pred_red).flatten() == 1] = False #remove samples of deforestation smaller than 6.25ha

            predicted_samples = pred_b.flatten()[keep_samples]
            label_samples = label.flatten()[keep_samples]

            tp = np.logical_and(predicted_samples == 1, label_samples == 1).sum()
            tn = np.logical_and(predicted_samples == 0, label_samples == 0).sum()
            fp = np.logical_and(predicted_samples == 1, label_samples == 0).sum()
            fn = np.logical_and(predicted_samples == 0, label_samples == 1).sum()

            precision = tp / (tp + fp)
            recall = tp / (tp + fn)
            f1 = 2 * precision * recall / (precision + recall)

            print(f'Image Year 0: {im_0}, Image Year 1: {im_1}, Pecision: {precision:.6f}, Recall: {recall:.6f}, F1-Score: {f1:.6f}')
            print(f'Image Year 0: {im_0}, Image Year 1: {im_1}, {tp:,}|{tn:,}|{fp:,}|{fn:,}')

            tps.append(tp)
            tns.append(tn)
            fps.append(fp)
            fns.append(fn)

            year_0 = str(args.year-1)[2:]
            year_1 = str(args.year)[2:]

            opt_files = os.listdir(paths.PREPARED_OPT_PATH)
            sar_files = os.listdir(paths.PREPARED_SAR_PATH)

            opt_files_0 = [fi for fi in opt_files if fi.startswith(year_0)]
            opt_file_0 = opt_files_0[im_0]
            opt_files_1 = [fi for fi in opt_files if fi.startswith(year_1)]
            opt_file_1 = opt_files_1[im_1]

            cmap_0 = load_sb_image(os.path.join(paths.GENERAL_PATH, f'{general.CMAP_PREFIX}_{opt_file_0[:-4]}.tif'))
            cmap_1 = load_sb_image(os.path.join(paths.GENERAL_PATH, f'{general.CMAP_PREFIX}_{opt_file_1[:-4]}.tif'))

            max_cmap = np.maximum(cmap_0, cmap_1)
            max_cmap = max_cmap.flatten()[keep_samples]

            cloud_args = max_cmap >= general.MIN_CLOUD_COVER

            cloud_predicted_samples = predicted_samples[cloud_args]
            cloud_label_samples = label_samples[cloud_args]

            no_cloud_predicted_samples = predicted_samples[np.logical_not(cloud_args)]
            no_cloud_label_samples = label_samples[np.logical_not(cloud_args)]

            cloud_tp = np.logical_and(cloud_predicted_samples == 1, cloud_label_samples == 1).sum()
            cloud_tn = np.logical_and(cloud_predicted_samples == 0, cloud_label_samples == 0).sum()
            cloud_fp = np.logical_and(cloud_predicted_samples == 1, cloud_label_samples == 0).sum()
            cloud_fn = np.logical_and(cloud_predicted_samples == 0, cloud_label_samples == 1).sum()

            no_cloud_tp = np.logical_and(no_cloud_predicted_samples == 1, no_cloud_label_samples == 1).sum()
            no_cloud_tn = np.logical_and(no_cloud_predicted_samples == 0, no_cloud_label_samples == 0).sum()
            no_cloud_fp = np.logical_and(no_cloud_predicted_samples == 1, no_cloud_label_samples == 0).sum()
            no_cloud_fn = np.logical_and(no_cloud_predicted_samples == 0, no_cloud_label_samples == 1).sum()

            c_tps.append(cloud_tp)
            c_tns.append(cloud_tn)
            c_fps.append(cloud_fp)
            c_fns.append(cloud_fn)

            nc_tps.append(no_cloud_tp)
            nc_tns.append(no_cloud_tn)
            nc_fps.append(no_cloud_fp)
            nc_fns.append(no_cloud_fn)

    print('end')
    tps = np.array(tps).sum()
    tns = np.array(tns).sum()
    fps = np.array(fps).sum()
    fns = np.array(fns).sum()

    c_tps = np.array(c_tps).sum()
    c_tns = np.array(c_tns).sum()
    c_fps = np.array(c_fps).sum()
    c_fns = np.array(c_fns).sum()

    nc_tps = np.array(nc_tps).sum()
    nc_tns = np.array(nc_tns).sum()
    nc_fps = np.array(nc_fps).sum()
    nc_fns = np.array(nc_fns).sum()

    precision = tps / (tps + fps)
    recall = tps / (tps + fns)
    f1 = 2 * precision * recall / (precision + recall)

    cloud_precision = c_tps / (c_tps + c_fps)
    cloud_recall = c_tps / (c_tps + c_fns)
    cloud_f1 = 2 * cloud_precision * cloud_recall / (cloud_precision + cloud_recall)

    no_cloud_precision = nc_tps / (nc_tps + nc_fps)
    no_cloud_recall = nc_tps / (nc_tps + nc_fns)
    no_cloud_f1 = 2 * no_cloud_precision * no_cloud_recall / (no_cloud_precision + no_cloud_recall)

    print(f'Global Results, Pecision: {precision:.6f}, Recall: {recall:.6f}, F1-Score: {f1:.6f}')
    print(f'Global Results, {tps:,}|{tns:,}|{fps:,}|{fns:,}')

    print(f'Cloud Results, Pecision: {cloud_precision:.6f}, Recall: {cloud_recall:.6f}, F1-Score: {cloud_f1:.6f}')
    print(f'Cloud Results, {c_tps:,}|{c_tns:,}|{c_fps:,}|{c_fns:,}')

    print(f'No Cloud Results, Pecision: {no_cloud_precision:.6f}, Recall: {no_cloud_recall:.6f}, F1-Score: {no_cloud_f1:.6f}')
    print(f'No Cloud Results, {nc_tps:,}|{nc_tns:,}|{nc_fps:,}|{nc_fns:,}')